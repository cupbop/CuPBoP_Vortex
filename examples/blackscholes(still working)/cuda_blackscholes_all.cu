// Amalgamated CUDA Black-Scholes source
// Generated by assistant on 2025-09-02T00:09:46.573560
// This file concatenates the original CUDA sources in the following order:
//   1) cudaBlackScholes_kernel.cu
//   2) cudaBlackScholes_launcher.cu
//   3) main.cu
// Header files (cuda_utils.h, cudaBlackScholes_common.h) are kept separate.
// No content has been removed; sections are copied verbatim.


// ===== BEGIN cudaBlackScholes_kernel.cu =====

#include "cuda_utils.h"
#include "cudaBlackScholes_common.h"
#include <cuda_runtime.h>

__device__ inline float cnd_approx_dev(float x) {
    const float inv_sqrt2 = 0.70710678118654752440f;
    // erfcf is available in device code
    return 0.5f * erfcf(-x * inv_sqrt2);
}

__global__ void black_scholes_kernel(
    float* d_call, float* d_put,
    const float* d_S, const float* d_K, const float* d_T,
    float R, float V, int n)
{
    int idx = blockDim.x * blockIdx.x + threadIdx.x;
    if (idx >= n) return;

    float S = d_S[idx];
    float K = d_K[idx];
    float T = d_T[idx];

    float sqrtT = sqrtf(T);
    float d1 = (logf(S / K) + (R + 0.5f * V * V) * T) / (V * sqrtT);
    float d2 = d1 - V * sqrtT;
    float CNDD1 = cnd_approx_dev(d1);
    float CNDD2 = cnd_approx_dev(d2);
    float expRT = __expf(-R * T);

    d_call[idx] = S * CNDD1 - K * expRT * CNDD2;
    d_put[idx]  = K * expRT * (1.0f - CNDD2) - S * (1.0f - CNDD1);
}

// Host launcher mirroring OpenCL "launcher" concept
extern "C" void BlackScholesGPU(
    float* h_call, float* h_put,
    const float* h_S, const float* h_K, const float* h_T,
    float R, float V, int n, int blockSize)
{
    float *d_call=nullptr, *d_put=nullptr, *d_S=nullptr, *d_K=nullptr, *d_T=nullptr;
    size_t bytes = sizeof(float) * n;

    CUDA_CHECK(cudaMalloc(&d_call, bytes));
    CUDA_CHECK(cudaMalloc(&d_put,  bytes));
    CUDA_CHECK(cudaMalloc(&d_S,    bytes));
    CUDA_CHECK(cudaMalloc(&d_K,    bytes));
    CUDA_CHECK(cudaMalloc(&d_T,    bytes));

    CUDA_CHECK(cudaMemcpy(d_S, h_S, bytes, cudaMemcpyHostToDevice));
    CUDA_CHECK(cudaMemcpy(d_K, h_K, bytes, cudaMemcpyHostToDevice));
    CUDA_CHECK(cudaMemcpy(d_T, h_T, bytes, cudaMemcpyHostToDevice));

    int grid = (n + blockSize - 1) / blockSize;
    black_scholes_kernel<<<grid, blockSize>>>(d_call, d_put, d_S, d_K, d_T, R, V, n);
    CUDA_CHECK(cudaGetLastError());
    CUDA_CHECK(cudaDeviceSynchronize());

    CUDA_CHECK(cudaMemcpy(h_call, d_call, bytes, cudaMemcpyDeviceToHost));
    CUDA_CHECK(cudaMemcpy(h_put,  d_put,  bytes, cudaMemcpyDeviceToHost));

    cudaFree(d_call);
    cudaFree(d_put);
    cudaFree(d_S);
    cudaFree(d_K);
    cudaFree(d_T);
}

// ===== END cudaBlackScholes_kernel.cu =====

// ===== BEGIN cudaBlackScholes_launcher.cu =====

#include "cuda_utils.h"
#include "cudaBlackScholes_common.h"

// Forward from kernel TU
extern "C" void BlackScholesGPU(
    float* h_call, float* h_put,
    const float* h_S, const float* h_K, const float* h_T,
    float R, float V, int n, int blockSize);

// OpenCL sample offered two entry points; provide compatible wrappers.

extern "C" void BlackScholes(
    float *CallResult, float *PutResult,
    float *StockPrice, float *OptionStrike, float *OptionYears,
    float Riskfree, float Volatility,
    unsigned options, unsigned groupSize, unsigned /*workItem*/)
{
    // Interpret groupSize as CUDA block size for a rough 1:1 mapping.
    int blockSize = (groupSize == 0) ? 256 : static_cast<int>(groupSize);
    BlackScholesGPU(CallResult, PutResult,
                    StockPrice, OptionStrike, OptionYears,
                    Riskfree, Volatility, static_cast<int>(options), blockSize);
}

extern "C" void closeBlackScholes(
    float *CallResult, float *PutResult,
    float *StockPrice, float *OptionStrike, float *OptionYears,
    float Riskfree, float Volatility,
    unsigned options, unsigned short /*useDouble*/)
{
    // In this CUDA port we only implement float, so ignore useDouble.
    BlackScholesGPU(CallResult, PutResult,
                    StockPrice, OptionStrike, OptionYears,
                    Riskfree, Volatility, static_cast<int>(options), 256);
}

// ===== END cudaBlackScholes_launcher.cu =====

// ===== BEGIN main.cu =====

// Minimal CUDA port of the OpenCL BlackScholes sample (1:1-ish API)
#include "cuda_utils.h"
#include "cudaBlackScholes_common.h"
#include <cstdio>
#include <vector>
#include <random>
#include <cstring>
#include <chrono>

extern "C" void BlackScholesGPU(
    float* h_call, float* h_put,
    const float* h_S, const float* h_K, const float* h_T,
    float R, float V, int n, int blockSize);

static void usage(const char* exe) {
    std::printf("Usage: %s [-n options] [-b blockSize] [-r riskfree] [-v volatility]\n", exe);
}

int main(int argc, char** argv) {
    int n = 1<<20;            // default 1M options
    int blockSize = 256;      // default CUDA block size
    float R = 0.05f;          // risk-free rate
    float V = 0.2f;           // volatility

    for (int i = 1; i < argc; ++i) {
        if (!std::strcmp(argv[i], "-n") && i+1 < argc) { n = std::atoi(argv[++i]); }
        else if (!std::strcmp(argv[i], "-b") && i+1 < argc) { blockSize = std::atoi(argv[++i]); }
        else if (!std::strcmp(argv[i], "-r") && i+1 < argc) { R = std::atof(argv[++i]); }
        else if (!std::strcmp(argv[i], "-v") && i+1 < argc) { V = std::atof(argv[++i]); }
        else if (!std::strcmp(argv[i], "-h") || !std::strcmp(argv[i], "--help")) { usage(argv[0]); return 0; }
    }

    std::vector<float> S(n), K(n), T(n), call(n), put(n), call_ref(n), put_ref(n);

    // Deterministic input for reproducibility
    std::mt19937 rng(1234);
    std::uniform_real_distribution<float> distS(5.0f, 50.0f);
    std::uniform_real_distribution<float> distK(10.0f, 25.0f);
    std::uniform_real_distribution<float> distT(0.25f, 10.0f);

    for (int i = 0; i < n; ++i) {
        S[i] = distS(rng);
        K[i] = distK(rng);
        T[i] = distT(rng);
    }

    auto t0 = std::chrono::high_resolution_clock::now();
    BlackScholesGPU(call.data(), put.data(),
                    S.data(), K.data(), T.data(),
                    R, V, n, blockSize);
    auto t1 = std::chrono::high_resolution_clock::now();
    double ms_gpu = std::chrono::duration<double, std::milli>(t1 - t0).count();

    // CPU "gold" reference (same math as OpenCL sample)
    t0 = std::chrono::high_resolution_clock::now();
    black_scholes_cpu(call_ref.data(), put_ref.data(),
                      S.data(), K.data(), T.data(),
                      R, V, n);
    t1 = std::chrono::high_resolution_clock::now();
    double ms_cpu = std::chrono::duration<double, std::milli>(t1 - t0).count();

    // Compute average absolute error
    double errC = 0.0, errP = 0.0;
    for (int i = 0; i < n; ++i) {
        errC += std::abs(call[i] - call_ref[i]);
        errP += std::abs(put[i]  - put_ref[i]);
    }
    errC /= n; errP /= n;

    std::printf("Options: %d, blockSize: %d\n", n, blockSize);
    std::printf("GPU time: %.3f ms (%.2f MOps/s)\n", ms_gpu, (n / 1e6) / (ms_gpu / 1e3));
    std::printf("CPU time: %.3f ms\n", ms_cpu);
    std::printf("Avg |err|  call: %.6f, put: %.6f\n", errC, errP);

    return 0;
}

// ===== END main.cu =====
